{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460ed58c",
   "metadata": {},
   "source": [
    "Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c15965",
   "metadata": {},
   "source": [
    "Perceptron Model\n",
    "- think about a biological neuron\n",
    "    - dendrites, nucleus, axon\n",
    "- they acept an input signal (dendrites) \n",
    "- calculation gets done (nucleus)\n",
    "- output is sent off (axon)\n",
    "\n",
    "- want the perceptron to be able to learn\n",
    "    - add weights onto the dendrites/ inputs and also add a bias term to the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0c766",
   "metadata": {},
   "source": [
    "Multi-layer perceptron model\n",
    "- fully connected neural network \n",
    "- every output is connected to the next input\n",
    "\n",
    "- first layer is the input layer\n",
    "- last layer is the output layer\n",
    "- middle layers are hidden layers\n",
    "\n",
    "deep neural network is when there are two or more hidden layers\n",
    "- width of a network is how many layers\n",
    "\n",
    "- can be used to approximate any convex continuous function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad1e1f",
   "metadata": {},
   "source": [
    "- in classification tasks it would be useful to have all outputs fall between 0 and 1\n",
    "- values can represent probability assignements for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187256",
   "metadata": {},
   "source": [
    "Activation Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec63d9c",
   "metadata": {},
   "source": [
    "- Can think of bias as being a term that x has to over come until x*w is greater than the bias, the effect wont overcome the bias after that then the effect is solely based on the value of w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f38a1",
   "metadata": {},
   "source": [
    "z= wx + b\n",
    "- we will refer to activation functions as f(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30830508",
   "metadata": {},
   "source": [
    "Sigmoid function has bounds 0 and 1\n",
    "- also known as the logistic function\n",
    "\n",
    "Hyperbolic tangent \n",
    "- bounds of -1 and 1\n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "- if output is less than 0 treat as 0 if greater than 0 output the actual value, especially good when dealing with vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7129f",
   "metadata": {},
   "source": [
    "Multi-Class Activation \n",
    "\n",
    "non-exclusive classes\n",
    "- a datapoint can have multiple class/categories assigned to it\n",
    "mutually exclusive class\n",
    "- a datapoint that can only have one class/category assigned to it\n",
    "\n",
    "organizing non-exclusive data\n",
    "- one-hot encoding also called dummy variables\n",
    "\n",
    "if non-exclusive can use sigmoid\n",
    "- allows each neuron to output independent of other classes\n",
    "\n",
    "if mutually exclusive use softmax\n",
    "- range 0,1\n",
    "- sum of all probabilities will be equal to one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78d438",
   "metadata": {},
   "source": [
    "Cost functions and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18aa59",
   "metadata": {},
   "source": [
    "also called loss functions or error functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e4578",
   "metadata": {},
   "source": [
    "how do we evaluate how far off the estiamtion is \n",
    "\n",
    "take estimated outputs and compare to real values of the label\n",
    "this occurs during fitting/training of the model\n",
    "\n",
    "cost function must be an average so it can output a single value\n",
    "keep track of loss/cost during trianing to monitor network during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bdb10",
   "metadata": {},
   "source": [
    "Quadratic cost function \n",
    "\n",
    "- a^L(x) is the last activation layer\n",
    "\n",
    "Cost function is four things\n",
    "- W- neural networks weights\n",
    "- B- biases\n",
    "- Sr- input of a single training sample\n",
    "- Er- desired output of that trianing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb6d7b",
   "metadata": {},
   "source": [
    "- want to minimize overall error\n",
    "- what weight will result in minimized cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c31d90",
   "metadata": {},
   "source": [
    "- it will be n-dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea452e",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "- calculate the slope at one point\n",
    "- move downward along the slope until you converge to 0 indidcating a min\n",
    "- larger step sizes are faster but you risk skipping over the minimum\n",
    "- this step size is called the learning rate\n",
    "    - learning rate is equal \n",
    "    \n",
    "Adaptive Gradient Descent\n",
    "- as you get closer to zero, the step size gets smaller\n",
    "- Adam is the best algorithm to use for this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae872ae",
   "metadata": {},
   "source": [
    "When dealing with n-dimensional vectors (tensors) notation changes from derivatives to gradients\n",
    "\n",
    "Cross entropy loss function\n",
    "- assumption is that your model predicts a probability distribution for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19677471",
   "metadata": {},
   "source": [
    "Backpropagation\n",
    "\n",
    "- moving backwards updating the weights and biases based on how to best minimize the cost function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3ca89",
   "metadata": {},
   "source": [
    "- each input has a weight and a bias\n",
    "- L-n ... , L-2, L-1, L\n",
    "- focusing on last two layers define z=wx+b\n",
    "- then apply activation function a=sigma(z)\n",
    "- how sensitive is the cost function to changes in w\n",
    "- take partial derivative of cost function with respect to the weights in network\n",
    "\n",
    "Steps\n",
    "- Step 1: using input x set the activation function a for the input layer\n",
    "- Step 2: for each layer compute z^(L) and a^(L)\n",
    "- Step 3: compute error vector\n",
    "- Step 4: Backpropagate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db3d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
